采用的两种激励函数分别为：

1. 逻辑函数(Log-Sigmoid)

    f(z) = 1/(1+exp(-z))

它的导数为：

    f'(z) = f(z)(1-f(z))



2. Tan-sigmoid

    f(z) = 2/(1+exp(-2z)) - 1

它的导数为：

    f'(z) = 1 - f(z)*f(z)

在训练模型时，采用的学习速率为0.5。在第一个隐藏层设置了100个神经元，在第二个隐藏层设置了50个神经元。在训练过程中，考虑到初始权重的随机性对模型带来的影响，将每个模型重复训练了5次。结果显示，当采用逻辑函数作为激励函数时，分类效果较好，在多次测试中正确分类率均在90%左右。而当采用tan-sigmoid函数作为激励函数时，分类效果不理想，在采用同样的迭代次数的情况下，模型很难将样本正确分类。

其中某一次的测试结果如下：

```
Under log-sigmoid function, the training error rate for training set is  7.526666666666666 %.
Under log-sigmoid function, the training error rate for testing set is  7.739999999999999 %.
Under log-sigmoid function, the training error rate for training set is  90.26333333333334 %.
Under log-sigmoid function, the training error rate for training set is  90.02 %.
```
